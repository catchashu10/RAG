{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "\n",
    "batch_size = 16\n",
    "epoch = 20\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "checkpoint_enable = True\n",
    "print(f\"Checkpoint: {checkpoint_enable}\")\n",
    "# model name : [\"all-MiniLM-L6-v2\", \"all-mpnet-base-v2\", \"multi-qa-mpnet-base-dot-v1\", \"all-distilroberta-v1\", \"all-MiniLM-L12-v2\", \"multi-qa-distilbert-cos-v1\", \"multi-qa-MiniLM-L6-cos-v1\"]\n",
    "model_output_folder = \"fine_tuned_models\"\n",
    "mode_output_name = f\"{model_name}_fineTuned_b{batch_size}_e{epoch}_bf16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a model to finetune:\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# 2. Load a dataset to finetune on\n",
    "file_path = \"/home/rishu/ashu/embeddingfinetuning/data/ike_qa_dataset.json\"\n",
    "datasets = get_datasets(\n",
    "    file_path, 0.9, 0\n",
    ")  # Train 90% Validation 0 and Test 10% [At the end of complete training]\n",
    "\n",
    "print_dataset_info(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define a loss function\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "\n",
    "# 4. Specify training arguments\n",
    "train_args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=f\"{model_output_folder}/{mode_output_name}\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=epoch,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    evaluation_strategy=\"no\",\n",
    "    # eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    log_on_each_node=False,\n",
    "    run_name=mode_output_name,  # Will be used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create an evaluator & evaluate the base model\n",
    "eval_dataset = datasets[\"evaluator_test\"]\n",
    "dev_evaluator = InformationRetrievalEvaluator(\n",
    "    eval_dataset[\"queries\"], eval_dataset[\"corpus\"], eval_dataset[\"relevant_docs\"]\n",
    ")\n",
    "\n",
    "print(f\"Evaluation for Base model {model_name}\\n: {dev_evaluator(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    # eval_dataset=datasets[\"val\"], # To get validation loss details during training. May slow down trainig.\n",
    "    loss=loss,\n",
    "    # evaluator=dev_evaluator, # To get detailed evaluation results during trainig.\n",
    ")\n",
    "trainer.train(resume_from_checkpoint=checkpoint_enable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Evaluate the trained model on the test set\n",
    "dev_evaluator(model)\n",
    "print(f\"Evaluation for FineTuned model {model_name}\\n: {dev_evaluator(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Save the trained model\n",
    "model.save_pretrained(\n",
    "    f\"{model_output_folder}/{mode_output_name}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
